{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow this tutorial:\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html\n",
    "\n",
    "\n",
    "Note that : \n",
    "\n",
    "- tensorboard in pytorch and tensorboard in tensorflow might cause conflict so make sure you install Pytorch and tensorboard only in your virtual environment\n",
    "- install the lastest version of tensorboard > 2.2 to have the projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# datasets\n",
    "trainset = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform)\n",
    "testset = torchvision.datasets.FashionMNIST('./data',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform)\n",
    "\n",
    "# dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                        shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "# constant for classes\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
    "\n",
    "# helper function to show an image\n",
    "# (used in the `plot_classes_preds` function below)\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TensorBoard setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/fashion_mnist_experiment_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Writing to TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# create grid of images\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# show images\n",
    "matplotlib_imshow(img_grid, one_channel=True)\n",
    "\n",
    "# write to tensorboard\n",
    "writer.add_image('four_fashion_mnist_images', img_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inspect the model using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_graph(net, images)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Adding a “Projector” to TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import tensorboard as tb\n",
    "# tf.io.gfile = tb.compat.tensorflow_stub.io.gfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "# helper function\n",
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    Selects n random datapoints and their corresponding labels from a dataset\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "# select random images and their target indices\n",
    "images, labels = select_n_random(trainset.data, trainset.targets)\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [classes[lab] for lab in labels]\n",
    "\n",
    "# log embeddings\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images.unsqueeze(1))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[preds[idx]],\n",
    "            probs[idx] * 100.0,\n",
    "            classes[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-0ee04cda3369>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# forward + backward + optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tkdang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-e7aeb1ea649c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tkdang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tkdang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tkdang\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    415\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 416\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "running_loss = 0.0\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize, update theta with mini-batches size = 4\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # every 1000 mini-batches...\n",
    "\n",
    "            # ...log the running loss\n",
    "            writer.add_scalar('training loss',\n",
    "                            running_loss / 1000,\n",
    "                            epoch * len(trainloader) + i)\n",
    "\n",
    "            # ...log a Matplotlib Figure showing the model's predictions on a\n",
    "            # random mini-batch\n",
    "            writer.add_figure('predictions vs. actuals',\n",
    "                            plot_classes_preds(net, inputs, labels),\n",
    "                            global_step=epoch * len(trainloader) + i)\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
